{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e46dc0",
   "metadata": {},
   "source": [
    "# Distributed Data Parallel RoBERTa Training with PyTorch and SageMaker distributed\n",
    "\n",
    "[Amazon SageMaker's distributed library](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html) can be used to train deep learning models faster and cheaper. The [data parallel](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html) feature in this library (`smdistributed.dataparallel`) is a distributed data parallel training framework for PyTorch, TensorFlow, and MXNet.\n",
    "\n",
    "This notebook demonstrates how to use `smdistributed.dataparallel` with PyTorch(version 1.9.0) on [Amazon SageMaker](https://aws.amazon.com/sagemaker/) to train a [faiseq RoBERTa model](https://github.com/HerringForks/fairseq/tree/use_herring) on [the WikiText-103 dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) using [Amazon FSx for Lustre file-system](https://aws.amazon.com/fsx/lustre/) as data source.\n",
    "\n",
    "The outline of steps is as follows:\n",
    "\n",
    "1. Stage the dataset in [Amazon S3](https://aws.amazon.com/s3/)\n",
    "2. Create Amazon FSx Lustre file-system and import data into the file-system from S3\n",
    "3. Build Docker training image and push it to [Amazon ECR](https://aws.amazon.com/ecr/)\n",
    "4. Configure data input channels for SageMaker\n",
    "5. Configure hyper-prarameters\n",
    "6. Define training metrics\n",
    "7. Define training job, set distribution strategy to SMDataParallel and start training\n",
    "\n",
    "**NOTE:** With large training dataset, we recommend using [Amazon FSx](https://aws.amazon.com/fsx/) as the input file system for the SageMaker training job. FSx file input to SageMaker significantly cuts down training start up time on SageMaker because it avoids downloading the training data each time you start the training job (as done with S3 input for SageMaker training job) and provides good data read throughput.\n",
    "\n",
    "\n",
    "**NOTE:** This example requires SageMaker Python SDK v2.X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4753d7",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Initialization\n",
    "\n",
    "Initialize the notebook instance. Get the AWS Region and a SageMaker execution role.\n",
    "\n",
    "### SageMaker role\n",
    "\n",
    "The following code cell defines `role` which is the IAM role ARN used to create and run SageMaker training and hosting jobs. This is the same IAM role used to create this SageMaker Notebook instance. \n",
    "\n",
    "`role` must have permission to create a SageMaker training job and host a model. For granular policies you can use to grant these permissions, see [Amazon SageMaker Roles](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). If you do not require fine-tuned permissions for this demo, you can use the IAM managed policy AmazonSageMakerFullAccess to complete this demo. \n",
    "\n",
    "As described above, since we will be using FSx, please make sure to attach `FSx Access` permission to this IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036033f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! python3 -m pip install --upgrade sagemaker\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "role_name = role.split([\"/\"][-1])\n",
    "print('------------------------------------------------')\n",
    "print(f\"SageMaker Execution Role:{role}\")\n",
    "print(f\"The name of the Execution role: {role_name[-1]}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account:{account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region:{region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e873f",
   "metadata": {},
   "source": [
    "To verify that the role above has required permissions:\n",
    "\n",
    "1. Go to the IAM console: https://console.aws.amazon.com/iam/home.\n",
    "2. Select **Roles**.\n",
    "3. Enter the role name in the search box to search for that role. \n",
    "4. Select the role.\n",
    "5. Use the **Permissions** tab to verify this role has required permissions attached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8b069",
   "metadata": {},
   "source": [
    "## Prepare SageMaker Training Images\n",
    "\n",
    "1. SageMaker by default uses the latest [Amazon Deep Learning Container Images (DLC)](https://github.com/aws/deep-learning-containers/blob/master/available_images.md) PyTorch training image. In this step, we use the DLC PyTorch 1.9 image as the base image and install additional dependencies required for training the RoBERTa model.\n",
    "2. We have forked the Facebook [Fairseq repository](https://github.com/pytorch/fairseq) to [HerringForks/fairseq](https://github.com/HerringForks/fairseq/tree/use_herring) so that we can make custom adaptations to the training script to make it work with `smdistributed.dataparallel`. Please refer to [this commit](https://github.com/HerringForks/fairseq/commit/173a6bf51cb251a787bac1b7620f753405071ed4) to see the actual code changes. We will use this fork to build the training docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568105b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the training image\n",
    "image = \"smddp-reborta\"\n",
    "tag = \"pt1.9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8cdc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the Docker file\n",
    "!pygmentize ./Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the script to build the training image and push it to ECR\n",
    "!pygmentize ./build_and_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login for DLC ECR account\n",
    "!aws ecr get-login-password --region {region} | docker login \\\n",
    "  --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and push the training image\n",
    "! chmod +x build_and_push.sh; bash build_and_push.sh {region} {image} {tag}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8af096",
   "metadata": {},
   "source": [
    "## Preparing FSx Input for SageMaker\n",
    "\n",
    "1. Download and prepare your training dataset on S3. One example dataset is [WikiText-103](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/). Instructions on preparing the WikiText-103 dataset can be found at [here](https://github.com/HerringForks/fairseq/blob/master/examples/roberta/README.pretraining.md)\n",
    "2. Follow the [steps](https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-fs-linked-data-repo.html) to create a FSx linked with your S3 bucket with training data. Make sure to add an endpoint to your VPC allowing S3 access.\n",
    "3. Follow the [steps](https://aws.amazon.com/blogs/machine-learning/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems/) to configure your SageMaker training job to use FSx.\n",
    "\n",
    "### Important Caveats\n",
    "\n",
    "1. You need to use the same `subnet` and `vpc` and `security group` used with FSx when launching the SageMaker notebook instance. The same configurations will be used by your SageMaker training job.\n",
    "2. Make sure you set [appropriate inbound/output rules](https://docs.aws.amazon.com/fsx/latest/LustreGuide/limit-access-security-groups.html) in the `security group`. Specifically, opening up these ports is necessary for SageMaker to access the FSx file system in the training job. \n",
    "3. Make sure `SageMaker IAM Role` used to launch this SageMaker training job has access to `AmazonFSx`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9400435",
   "metadata": {},
   "source": [
    "## Preparing Training Script\n",
    "To start training, SageMaker requires a single Python script as the entry point in each process/GPU. This script can either be the training script itself, or it can call other executables in the training docker container. In this example, we will have a entry point script that demonstrates the second case. Please remember to fill in the dataset directory in the entry point script. This is the directory to the dataset in your FSx file server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31de968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the entry point script\n",
    "!pygmentize ./entry_point.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f406c05",
   "metadata": {},
   "source": [
    "## SageMaker PyTorch Estimator function options\n",
    "\n",
    "In the following code block, you can update the estimator function to use a different instance type, instance count, and distribution strategy. You will also need to pass in your entry point script from above.\n",
    "\n",
    "**Instance types**\n",
    "\n",
    "SMDataParallel supports model training on SageMaker with the following instance types only. For best performance, it is recommended you use an instance type that supports Amazon Elastic Fabric Adapter (ml.p3dn.24xlarge and ml.p4d.24xlarge).\n",
    "\n",
    "1. ml.p3.16xlarge\n",
    "1. ml.p3dn.24xlarge [Recommended]\n",
    "1. ml.p4d.24xlarge [Recommended]\n",
    "\n",
    "**Instance count**\n",
    "\n",
    "To get the best performance and the most out of SMDataParallel, you should use at least 2 instances, but you can also use 1 for testing this example.\n",
    "\n",
    "**Distribution strategy**\n",
    "\n",
    "Note that to use DDP mode, you need to update the `distribution` strategy, and set it to use `smdistributed dataparallel`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c556d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9813eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.p4d.24xlarge' # Other supported instance type: ml.p3.16xlarge, ml.p3dn.24xlarge\n",
    "instance_count = 1  # You can use 2, 4, 8 etc.\n",
    "docker_image = f\"{account}.dkr.ecr.{region}.amazonaws.com/{image}:{tag}\"  # YOUR_ECR_IMAGE_BUILT_WITH_ABOVE_DOCKER_FILE\n",
    "username = \"AWS\"\n",
    "subnets = [\"<subnet-id>]  # Should be same as Subnet used for FSx. Example: subnet-0f9XXXX\n",
    "security_group_ids = [\n",
    "    \"<security-group-id>\"\n",
    "]  # Should be same as Security group used for FSx. sg-03ZZZZZZ\n",
    "file_system_id = \"<fsx-id>\"  # FSx file system ID with your training dataset. Example: 'fs-0bYYYYYY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50626abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FSx Input for your SageMaker Training job\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "file_system_directory_path = \"<fsx-mount-name>\"  # This is the mount name of the fsx\n",
    "file_system_access_mode = \"ro\"\n",
    "file_system_type = \"FSxLustre\"\n",
    "train_fs = FileSystemInput(\n",
    "    file_system_id=file_system_id,\n",
    "    file_system_type=file_system_type,\n",
    "    directory_path=file_system_directory_path,\n",
    "    file_system_access_mode=file_system_access_mode,\n",
    ")\n",
    "data_channels = {\"train\": train_fs} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure hyper-parameters\n",
    "# RoBERTa Large 1.3B parameters\n",
    "hyperparameters = {\n",
    "    'fp16': '',\n",
    "    'task': 'masked_lm',\n",
    "    'criterion': 'masked_lm',\n",
    "    'arch': 'roberta_large',\n",
    "    'sample-break-mode': 'complete',\n",
    "    'tokens-per-sample': 512,\n",
    "    'optimizer': 'adam',\n",
    "    'adam-eps': 1e-6,\n",
    "    'clip-norm': 0.0,\n",
    "    'lr-scheduler': 'polynomial_decay',\n",
    "    'lr': 0.0001,\n",
    "    'warmup-updates': 10000,\n",
    "    'total-num-update': 125000,\n",
    "    'dropout': 0.1,\n",
    "    'attention-dropout': 0.1,\n",
    "    'weight-decay': 0.01,\n",
    "    'max-sentences': 8,\n",
    "    'update-freq': 1,\n",
    "    'max-update': 125000,\n",
    "    'log-format': 'simple',\n",
    "    'log-interval': 10,\n",
    "    'encoder-layers': 24,\n",
    "    'encoder-embed-dim': 2048,\n",
    "    'encoder-ffn-embed-dim': 8192,\n",
    "    'memory-efficient-fp16': '',\n",
    "    'distributed-no-spawn' : '',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96091ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure hyper-parameters\n",
    "# RoBERTa Large 350M parameters\n",
    "hyperparameters = {\n",
    "    'fp16': '',\n",
    "    'task': 'masked_lm',\n",
    "    'criterion': 'masked_lm',\n",
    "    'arch': 'roberta_large',\n",
    "    'sample-break-mode': 'complete',\n",
    "    'tokens-per-sample': 512,\n",
    "    'optimizer': 'adam',\n",
    "    'adam-eps': 1e-6,\n",
    "    'clip-norm': 0.0,\n",
    "    'lr-scheduler': 'polynomial_decay',\n",
    "    'lr': 0.0001,\n",
    "    'warmup-updates': 10000,\n",
    "    'total-num-update': 125000,\n",
    "    'dropout': 0.1,\n",
    "    'attention-dropout': 0.1,\n",
    "    'weight-decay': 0.01,\n",
    "    'max-sentences': 16,\n",
    "    'update-freq': 1,\n",
    "    'max-update': 125000,\n",
    "    'log-format': 'simple',\n",
    "    'log-interval': 10,\n",
    "    'distributed-no-spawn' : ''\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"pytorch-smdataparallel-roberta-testrun-new-16\"  # This job name is used as prefix to the sagemaker training job. Makes it easy to look for your training job in SageMaker Training job console.\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"entry_point.py\",\n",
    "    role=role,\n",
    "    image_uri=docker_image,\n",
    "    source_dir=\".\",\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters=hyperparameters,\n",
    "    subnets=subnets,\n",
    "    security_group_ids=security_group_ids,\n",
    "    debugger_hook_config=False,\n",
    "    # Training using SMDataParallel Distributed Training Framework\n",
    "    distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}},\n",
    ")\n",
    "# Submit SageMaker training job\n",
    "estimator.fit(inputs=data_channels, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d412cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
